

<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
	<style>
		/* THIS IS FOR LAYOUT (media query specific) */
		/*This applies from 0px to 600px*/
		
		body {
		  /*background: red;*/
		}

		.container {
			width: 100%;
		}

		/*This applies from 600px onwards*/
		
		@media (min-width: 600px) {
		  body {
			/*background: green;*/
		  }
		  
		  .container {
			margin-left: auto;
			margin-right: auto;
			width: 600px;
		  }
		  
		}
	</style>
	
	<style>
		/* THIS IS FOR AESTHETICS */
		
		/* ok I lied, here's a tiny bit of structure for canvas */
		.container__myCanvas {
			/*
			width: 300px;
			height: 300px;
			*/
			display: block;
			margin: 0 auto 20px auto;
			
		}
		
		.container__title {
			text-align: center;
		}
		
		
		body {
			/*color: white;*/
			font-family: Arial, Helvetica, sans-serif;
		}
		
		.buttons {
			/*
			The buttons are inline with a block parent (div), so you can
			just use text-align: center to center the buttons.
			*/
			text-align: center;

		}
		
		.buttons__btn {
			background: #329ea8;
			color: white;
			border: none;
			padding: 0.75em;
			border-radius: 5px;
			font-weight: bold;
			margin-left: 5px;
			margin-right: 5px;
			width: 150px;
		}
		
		.container__results {
			margin-top: 5px;
			margin-bottom: 5px;
		}
		
		.container__results--predictionvalue, .container__results--confidencevalue, .container__results--modelversion {
			font-weight: bold;
		}
		
		
		#loading {
			position: fixed; /* or absolute */
			top: 50%;
			left: 50%;
			margin-top: -32px;
			margin-left: -32px;
			display: none;
		}
		
		
	</style>
</head>
<body>
	<img id="loading" src="loading.gif"></img>
	<div class="container">
		<h1 class="container__title">PyNumDraw!</h1>
		<p>This website uses machine learning to guess what number you drew below. Draw a number between 0 and 9 in the middle of the canvas below and then click "predict"!</p>
		<canvas class="container__myCanvas">
		  <p>Add suitable fallback here.</p>
		</canvas>
		<div class="buttons">
			<button id="btn_submit_canvas" class="buttons__btn buttons__btn--predict">Predict</button>
			<button id="btn_clear_canvas" class="buttons__btn buttons__btn--clear">Clear canvas</button>
		</div>
		<p class="container__results">Prediction: <span id="prediction_output_value" class="container__results--predictionvalue"></span></p>
		<p class="container__results">Prediction Confidence: <span id="prediction_output_confidence" class="container__results--confidencevalue"></span></p>
		<p class="container__results">Model Version: <span id="prediction_model_version" class="container__results--modelversion"></span></p>
		<p>Limitations:</p>
		<ul>
			<li>The app is bad at predicting "9". Many of the hand-written training examples of "9" actually look like lower-case "a". If you draw a lower case "a", chances are it will confidently predict "9".</li>
			<li>"Confidence" is really just the value from the last soft-max operation on the fully connected class output layer. This means that there are instances of a high confidence value even when the input to the model is clearly nothing similar to any of the training examples it's seen so far.</li>
		</ul>

	</div>

    <script>

		// ================== codepen https://codepen.io/mero789/pen/qBdYWxY ===============
		// (also canvas drawing setup, but this one will support mobile devices as well)
		const canvas = document.querySelector(".container__myCanvas");
		const ctx = canvas.getContext("2d");
		
		var span_predict_value = document.querySelector('#prediction_output_value');
		var span_predict_confidence = document.querySelector('#prediction_output_confidence');
		var span_predict_model_version = document.querySelector('#prediction_model_version');
		const clearBtn = document.querySelector('#btn_clear_canvas');
		const predictBtn = document.querySelector('#btn_submit_canvas');
		const load_icon = document.querySelector("#loading");
		

		const width  = canvas.width = 500;
		const height = canvas.height = 500;

		let color = "#ffffff";  // should only be white
		let strokeSize = 20;


		// ========= handle resize and zooming =====================
		
		//for zoom detection
		px_ratio = window.devicePixelRatio || window.screen.availWidth / document.documentElement.clientWidth;

		window.addEventListener('resize', isZooming);

		function isZooming(){
			
			getCanvasRectLocation();
			
			var newPx_ratio = window.devicePixelRatio || window.screen.availWidth / document.documentElement.clientWidth;
			if(newPx_ratio != px_ratio){
				// changing zoom level in browser
				console.log("zoom level is changing");	
				return true;
			}else{
				// just resizing the window
				console.log("window is being resized");
				return false;
			}
		};


		// TJV, adding back in my logic for setting global variables for global width/height
		function getCanvasRectLocation() {
			console.log("window has been loaded and/or resized");
			window.canvas_rect = canvas.getBoundingClientRect();
			load_icon.style.top  = (window.canvas_rect.top + (height/2)).toString().concat("px");
			load_icon.style.left = (window.canvas_rect.left + (width/2)).toString().concat("px");
			
		}
		getCanvasRectLocation()



		ctx.fillStyle = 'rgb(0,0,0)';
		ctx.fillRect(0,0,width,height);

		//variables
		let painting = false;

		//functions
		function startPosition(e) {
			painting = true;
			draw(e);
		}

		function endPosition() {
			painting = false;
			ctx.beginPath();
		}

		function draw(e) {
			if (!painting) {
				return;
			}
			e.preventDefault();
			e.stopPropagation();
			ctx.lineWidth = strokeSize;
			ctx.lineCap = "round";
			//ctx.lineTo(e.clientX, e.clientY);
			ctx.lineTo((e.clientX - window.canvas_rect.left), (e.clientY - window.canvas_rect.top));
			ctx.stroke();
			ctx.strokeStyle = color;
			ctx.beginPath();
			//ctx.moveTo(e.clientX, e.clientY);
			ctx.moveTo((e.clientX - window.canvas_rect.left), (e.clientY - window.canvas_rect.top));
		}

		//event listeners
		canvas.addEventListener("mousedown", startPosition);
		canvas.addEventListener("touchstart", startPosition);
		canvas.addEventListener("mouseup", endPosition);
		canvas.addEventListener("touchend", endPosition);
		canvas.addEventListener("mousemove", draw);
		
		// this is equivalent of "draw" but for browsers on mobile phones
		canvas.addEventListener("touchmove", function (e) {
			var touch = e.touches[0];
			var mouseEvent = new MouseEvent("mousemove", {
				clientX: touch.clientX,
				clientY: touch.clientY
			});
			draw(mouseEvent);
		}, false);
		
		
		
		// =========== data processing and button click stuff ======================

	
		predictBtn.onclick = function() {
			console.log("predict button was clicked! wahoo!");
			
			imgdata = ctx.getImageData(0, 0, width, width);
			console.log("starting loop...");
			console.log(imgdata);
			
			var prepared_array = [];	
			
			// it's a gray-scale image, so just grab all the red pixels to 
			// reduce how much data we have to pass to our API
			for (var i = 0; i < imgdata.data.length; i += 4) {
				prepared_array.push(imgdata.data[i] / 255);      // red
				//imgdata[i + 1] = 255 - imgdata[i + 1]; 			// green
				//imgdata[i + 2] = 255 - imgdata[i + 2]; 			// blue
			}
			console.log("done with loop.");
			
			console.log(prepared_array);
			
			console.log('calling our send_data function now');
			send_data_for_prediction(prepared_array);
		}


		send_data_for_prediction = function(p_data) {
			loading.style.display = "block";
			console.log('send_data_for_prediction has been called!');
			var xmlhttp = new XMLHttpRequest();   // new HttpRequest instance 
			var theUrl = "/predict_number/";
			xmlhttp.responseType = 'json';
			xmlhttp.open("POST", theUrl);
			xmlhttp.setRequestHeader("Content-Type", "application/json;charset=UTF-8");
			xmlhttp.send(JSON.stringify({ "new_data": p_data }));
			console.log("and heres the response from python:");
		
			xmlhttp.onload  = function() {
			
				var jsonResponse = xmlhttp.response;
				// do something with jsonResponse
				span_predict_value.innerHTML = jsonResponse.predicted_number;
				span_predict_confidence.innerHTML = jsonResponse.confidence.concat("%");
				span_predict_model_version.innerHTML = jsonResponse.model_version;
				console.log(jsonResponse.model_version);
				console.log(jsonResponse);
				loading.style.display = "none";
			};
		}
		
		clearBtn.onclick = function() {
			ctx.fillStyle = 'rgb(0,0,0)';
			ctx.fillRect(0,0,width,height);
			
			// remove the prior predictions
			span_predict_value.innerHTML = "";
			span_predict_confidence.innerHTML = "";
			span_predict_model_version.innerHTML = "";
		}
			
				

		/*
		TJV notes: so the way I'm interpreting this is...
		
		Desktop browser:
		1. user clicks in the canvas and holds down mouse
		2. this triggers "mousedown" event, so "startPosition" is called
		3. start position simply sets `painting` variable to true and passes the event to `draw()` function
		4. draw handles the actual... drawing, using <event>.clientX and <event>.clientY
		
		Mobile browser:
		* The exact same, except there's a little helper function in between "touchmove" event and "draw()"
		* because the mobile browser path uses the same draw function, we have nothing left to edit here
		
		So I think all I need to do is calculate the top left corner of canvas and subtract
		that from the clientX and clientY variables respectively.
		
		*/


    </script>


</body>
</html>



